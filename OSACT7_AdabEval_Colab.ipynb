{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# OSACT7 AdabEval - Task A: Arabic Politeness Detection\n",
    "\n",
    "Ce notebook permet de classifier des textes arabes en trois categories:\n",
    "- **Polite** (Poli)\n",
    "- **Impolite** (Impoli)\n",
    "- **Neutral** (Neutre)\n",
    "\n",
    "**Meilleur modele:** MARBERT avec Focal Loss (F1 = 0.84)\n",
    "\n",
    "---\n",
    "\n",
    "## Instructions\n",
    "1. Assurez-vous d'avoir un GPU active (Runtime > Change runtime type > GPU)\n",
    "2. Uploadez vos fichiers CSV (`TaskApoliteness_train.csv`, `TaskApoliteness_val.csv`) ou utilisez Google Drive\n",
    "3. Executez les cellules dans l'ordre"
   ],
   "metadata": {
    "id": "header"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Configuration et Installation"
   ],
   "metadata": {
    "id": "setup_section"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# Installation des dependances\n",
    "!pip install -q transformers datasets accelerate\n",
    "!pip install -q arabert\n",
    "!pip install -q xgboost imbalanced-learn\n",
    "!pip install -q seaborn"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Verification GPU\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA disponible: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memoire GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ],
   "metadata": {
    "id": "check_gpu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, AutoModelForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "try:\n",
    "    from arabert.preprocess import ArabertPreprocessor\n",
    "except ImportError:\n",
    "    ArabertPreprocessor = None\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "SEED = 42\n",
    "NUM_LABELS = 3\n",
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE = 16\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "print(f\"Device: {DEVICE}\")"
   ],
   "metadata": {
    "id": "imports"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Chargement des Donnees\n",
    "\n",
    "**Option A:** Upload direct des fichiers CSV\n",
    "\n",
    "**Option B:** Montage Google Drive"
   ],
   "metadata": {
    "id": "data_section"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Option A: Upload des fichiers\n",
    "from google.colab import files\n",
    "\n",
    "USE_DRIVE = False  # Mettre True pour utiliser Google Drive\n",
    "\n",
    "if not USE_DRIVE:\n",
    "    print(\"Uploadez vos fichiers CSV (TaskApoliteness_train.csv et TaskApoliteness_val.csv)\")\n",
    "    uploaded = files.upload()\n",
    "    TRAIN_CSV = \"TaskApoliteness_train.csv\"\n",
    "    VAL_CSV = \"TaskApoliteness_val.csv\"\n",
    "else:\n",
    "    # Option B: Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    DATA_DIR = \"/content/drive/MyDrive/abeval\"  # Modifiez ce chemin\n",
    "    TRAIN_CSV = f\"{DATA_DIR}/TaskApoliteness_train.csv\"\n",
    "    VAL_CSV = f\"{DATA_DIR}/TaskApoliteness_val.csv\""
   ],
   "metadata": {
    "id": "load_data_option"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Chargement des donnees\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "val_df = pd.read_csv(VAL_CSV)\n",
    "\n",
    "print(f\"Donnees d'entrainement: {len(train_df)} exemples\")\n",
    "print(f\"Donnees de validation: {len(val_df)} exemples\")\n",
    "print(f\"\\nDistribution des classes (train):\")\n",
    "print(train_df['label'].value_counts())"
   ],
   "metadata": {
    "id": "load_data"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Visualisation de la distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "train_df['label'].value_counts().plot(kind='bar', ax=axes[0], color=['#3498db', '#e74c3c', '#2ecc71'])\n",
    "axes[0].set_title('Distribution Train')\n",
    "axes[0].set_ylabel('Nombre')\n",
    "\n",
    "val_df['label'].value_counts().plot(kind='bar', ax=axes[1], color=['#3498db', '#e74c3c', '#2ecc71'])\n",
    "axes[1].set_title('Distribution Validation')\n",
    "axes[1].set_ylabel('Nombre')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "visualize_distribution"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Preprocessing"
   ],
   "metadata": {
    "id": "preprocessing_section"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def preprocess_text(text, arabert_prep=None):\n",
    "    \"\"\"Preprocesse le texte arabe.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    if arabert_prep is not None:\n",
    "        try:\n",
    "            text = arabert_prep.preprocess(text)\n",
    "        except Exception:\n",
    "            pass\n",
    "    # Nettoyage\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    # Normalisation arabe\n",
    "    text = re.sub(r'[إأآا]', 'ا', text)\n",
    "    text = re.sub(r'ة', 'ه', text)\n",
    "    text = re.sub(r'ى', 'ي', text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "# Encodage des labels\n",
    "le = LabelEncoder()\n",
    "train_df['label_encoded'] = le.fit_transform(train_df['label'])\n",
    "val_df['label_encoded'] = le.transform(val_df['label'])\n",
    "\n",
    "label_map = {int(i): str(l) for i, l in enumerate(le.classes_)}\n",
    "print(f\"Mapping des labels: {label_map}\")\n",
    "\n",
    "y_train = train_df['label_encoded'].values\n",
    "y_val = val_df['label_encoded'].values\n",
    "\n",
    "# Calcul des poids de classe\n",
    "def compute_class_weights(labels, num_classes):\n",
    "    counts = np.bincount(labels, minlength=num_classes).astype(float)\n",
    "    weights = len(labels) / (num_classes * counts)\n",
    "    return torch.tensor(weights, dtype=torch.float32)\n",
    "\n",
    "class_weights = compute_class_weights(y_train, NUM_LABELS)\n",
    "print(f\"Poids des classes: {class_weights.tolist()}\")"
   ],
   "metadata": {
    "id": "preprocessing"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Definition du Modele"
   ],
   "metadata": {
    "id": "model_section"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Focal Loss - Meilleur pour les donnees desequilibrees\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Focal Loss: reduit le poids des exemples bien classifies,\n",
    "    se concentre sur les exemples difficiles.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=None, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        ce_loss = F.cross_entropy(logits, targets, weight=self.alpha, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "# Dataset PyTorch\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=MAX_LENGTH):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(\n",
    "            self.texts[idx], padding='max_length', truncation=True,\n",
    "            max_length=self.max_length, return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': enc['input_ids'].squeeze(0),\n",
    "            'attention_mask': enc['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }"
   ],
   "metadata": {
    "id": "model_definition"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Entrainement\n",
    "\n",
    "Utilisation de **MARBERT** (entraine sur 1B de tweets arabes)"
   ],
   "metadata": {
    "id": "training_section"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Configuration du modele\n",
    "MODEL_NAME = \"UBC-NLP/MARBERT\"  # Meilleur modele pour les tweets arabes\n",
    "\n",
    "print(f\"Chargement du tokenizer: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Preprocessing des textes\n",
    "print(\"Preprocessing des textes...\")\n",
    "train_texts = train_df['Sentence'].apply(preprocess_text).tolist()\n",
    "val_texts = val_df['Sentence'].apply(preprocess_text).tolist()\n",
    "\n",
    "# Creation des datasets\n",
    "train_dataset = TextDataset(train_texts, y_train, tokenizer)\n",
    "val_dataset = TextDataset(val_texts, y_val, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f\"Batches train: {len(train_loader)}, val: {len(val_loader)}\")"
   ],
   "metadata": {
    "id": "prepare_data"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Fonctions d'entrainement\n",
    "def train_one_epoch(model, loader, optimizer, scheduler, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    for batch in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        ids = batch['input_ids'].to(device)\n",
    "        mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=ids, attention_mask=mask)\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item() * len(labels)\n",
    "        all_preds.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    _, _, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    return avg_loss, f1\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    for batch in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "        ids = batch['input_ids'].to(device)\n",
    "        mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=ids, attention_mask=mask)\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "\n",
    "        total_loss += loss.item() * len(labels)\n",
    "        all_preds.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    return avg_loss, acc, prec, rec, f1, np.array(all_preds), np.array(all_labels)"
   ],
   "metadata": {
    "id": "training_functions"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Entrainement du modele\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ENTRAINEMENT: {MODEL_NAME}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Chargement du modele\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, num_labels=NUM_LABELS\n",
    ").to(DEVICE)\n",
    "\n",
    "# Loss et Optimizer\n",
    "criterion = FocalLoss(alpha=class_weights.to(DEVICE), gamma=2.0)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "\n",
    "NUM_EPOCHS = 15\n",
    "total_steps = len(train_loader) * NUM_EPOCHS\n",
    "warmup_steps = int(total_steps * 0.1)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Boucle d'entrainement\n",
    "best_f1, best_epoch, patience = 0.0, 0, 0\n",
    "history = {'train_loss': [], 'val_loss': [], 'train_f1': [], 'val_f1': []}\n",
    "best_preds = None\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    print(f\"\\nEpoch {epoch}/{NUM_EPOCHS}\")\n",
    "\n",
    "    t_loss, t_f1 = train_one_epoch(model, train_loader, optimizer, scheduler, criterion, DEVICE)\n",
    "    v_loss, v_acc, v_prec, v_rec, v_f1, v_preds, v_labels = evaluate(model, val_loader, criterion, DEVICE)\n",
    "\n",
    "    history['train_loss'].append(t_loss)\n",
    "    history['val_loss'].append(v_loss)\n",
    "    history['train_f1'].append(t_f1)\n",
    "    history['val_f1'].append(v_f1)\n",
    "\n",
    "    print(f\"  Train - Loss: {t_loss:.4f}, F1: {t_f1:.4f}\")\n",
    "    print(f\"  Val   - Loss: {v_loss:.4f}, F1: {v_f1:.4f}, Acc: {v_acc:.4f}\")\n",
    "\n",
    "    if v_f1 > best_f1:\n",
    "        best_f1 = v_f1\n",
    "        best_epoch = epoch\n",
    "        best_preds = v_preds.copy()\n",
    "        # Sauvegarde du modele\n",
    "        model.save_pretrained('best_model')\n",
    "        tokenizer.save_pretrained('best_model')\n",
    "        patience = 0\n",
    "        print(f\"  >> Nouveau meilleur modele (F1={v_f1:.4f})\")\n",
    "    else:\n",
    "        patience += 1\n",
    "        if patience >= 4:\n",
    "            print(f\"  >> Early stopping a l'epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"RESULTAT FINAL: F1 = {best_f1:.4f} (epoch {best_epoch})\")\n",
    "print(f\"{'='*60}\")"
   ],
   "metadata": {
    "id": "train_model"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Resultats et Visualisation"
   ],
   "metadata": {
    "id": "results_section"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Courbes d'entrainement\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history['train_loss'], label='Train', marker='o')\n",
    "axes[0].plot(history['val_loss'], label='Validation', marker='s')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Evolution de la Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# F1\n",
    "axes[1].plot(history['train_f1'], label='Train', marker='o')\n",
    "axes[1].plot(history['val_f1'], label='Validation', marker='s')\n",
    "axes[1].axvline(x=best_epoch-1, color='r', linestyle='--', label=f'Best (epoch {best_epoch})')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('F1 Score (Macro)')\n",
    "axes[1].set_title('Evolution du F1 Score')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves.png', dpi=150)\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "training_curves"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Matrice de confusion\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "cm = confusion_matrix(y_val, best_preds)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "            xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "ax.set_title(f'Matrice de Confusion (F1={best_f1:.4f})', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Predit')\n",
    "ax.set_ylabel('Reel')\n",
    "plt.savefig('confusion_matrix.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Rapport de classification\n",
    "print(\"\\nRapport de Classification:\")\n",
    "print(classification_report(y_val, best_preds, target_names=le.classes_, zero_division=0))"
   ],
   "metadata": {
    "id": "confusion_matrix"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7. Sauvegarde et Export"
   ],
   "metadata": {
    "id": "export_section"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Sauvegarde des predictions\n",
    "submission = pd.DataFrame({\n",
    "    'id': range(1, len(val_df) + 1),\n",
    "    'Sentence': val_df['Sentence'].values,\n",
    "    'True_Label': le.inverse_transform(y_val),\n",
    "    'Predicted_Label': le.inverse_transform(best_preds)\n",
    "})\n",
    "submission.to_csv('predictions.csv', index=False, encoding='utf-8-sig')\n",
    "print(f\"Predictions sauvegardees: predictions.csv ({len(submission)} lignes)\")\n",
    "\n",
    "# Sauvegarde de la configuration\n",
    "config = {\n",
    "    'model_name': MODEL_NAME,\n",
    "    'best_epoch': best_epoch,\n",
    "    'best_f1': float(best_f1),\n",
    "    'label_mapping': label_map,\n",
    "    'class_weights': class_weights.tolist()\n",
    "}\n",
    "with open('config.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(config, f, indent=2, ensure_ascii=False)\n",
    "print(\"Configuration sauvegardee: config.json\")"
   ],
   "metadata": {
    "id": "save_results"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Telecharger les fichiers\n",
    "from google.colab import files\n",
    "\n",
    "print(\"Telechargement des fichiers...\")\n",
    "files.download('predictions.csv')\n",
    "files.download('config.json')\n",
    "files.download('training_curves.png')\n",
    "files.download('confusion_matrix.png')\n",
    "\n",
    "# Pour telecharger le modele complet (optionnel - gros fichier)\n",
    "# !zip -r best_model.zip best_model/\n",
    "# files.download('best_model.zip')"
   ],
   "metadata": {
    "id": "download_files"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8. Inference sur de Nouvelles Donnees"
   ],
   "metadata": {
    "id": "inference_section"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def predict(texts, model, tokenizer, device):\n",
    "    \"\"\"Predit les labels pour une liste de textes.\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    for text in texts:\n",
    "        processed = preprocess_text(text)\n",
    "        enc = tokenizer(\n",
    "            processed, padding='max_length', truncation=True,\n",
    "            max_length=MAX_LENGTH, return_tensors='pt'\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=enc['input_ids'].to(device),\n",
    "                attention_mask=enc['attention_mask'].to(device)\n",
    "            )\n",
    "            pred = torch.argmax(outputs.logits, dim=1).item()\n",
    "            predictions.append(le.inverse_transform([pred])[0])\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# Exemple d'utilisation\n",
    "exemples = [\n",
    "    \"شكرا جزيلا على مساعدتك\",           # Merci beaucoup pour ton aide\n",
    "    \"هذا الشخص غبي جدا\",                # Cette personne est tres stupide\n",
    "    \"الطقس جميل اليوم\",                  # Le temps est beau aujourd'hui\n",
    "    \"لو سمحت ممكن تساعدني\",             # S'il te plait, peux-tu m'aider\n",
    "]\n",
    "\n",
    "# Charger le meilleur modele\n",
    "best_model = AutoModelForSequenceClassification.from_pretrained('best_model').to(DEVICE)\n",
    "best_tokenizer = AutoTokenizer.from_pretrained('best_model')\n",
    "\n",
    "# Predictions\n",
    "preds = predict(exemples, best_model, best_tokenizer, DEVICE)\n",
    "\n",
    "print(\"\\nExemples de predictions:\")\n",
    "print(\"-\" * 60)\n",
    "for text, pred in zip(exemples, preds):\n",
    "    print(f\"Texte: {text}\")\n",
    "    print(f\"Prediction: {pred}\")\n",
    "    print(\"-\" * 60)"
   ],
   "metadata": {
    "id": "inference"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## Resume\n",
    "\n",
    "Ce notebook implemente un systeme de detection de politesse en arabe utilisant:\n",
    "- **Modele:** MARBERT (entraine sur 1B de tweets arabes)\n",
    "- **Loss:** Focal Loss (meilleure pour les donnees desequilibrees)\n",
    "- **Performance attendue:** F1 ~ 0.84\n",
    "\n",
    "### Fichiers generes:\n",
    "- `predictions.csv` - Predictions sur les donnees de validation\n",
    "- `config.json` - Configuration du modele\n",
    "- `best_model/` - Modele entraine (HuggingFace format)\n",
    "- `training_curves.png` - Courbes d'entrainement\n",
    "- `confusion_matrix.png` - Matrice de confusion"
   ],
   "metadata": {
    "id": "summary"
   }
  }
 ]
}
